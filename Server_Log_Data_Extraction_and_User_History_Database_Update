{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuguvi26/guviproject/blob/main/Server_Log_Data_Extraction_and_User_History_Database_Update\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Source code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReadMe\n",
        "# User History Extraction & Analysis\n",
        "\n",
        "## Overview\n",
        "This project is designed to make sense of email activity recorded in server log files. Logs are usually unstructured and hard to analyze, so the goal here is to extract email addresses and timestamps, store them in databases, and run queries to get actionable insights. In short, it’s a small but complete data pipeline for tracking historical email activity.\n",
        "\n",
        "---\n",
        "\n",
        "## Problem\n",
        "Server logs contain valuable information about user activity, but the raw data is messy. This project solves that by:\n",
        "1. Extracting every email address along with its associated date and time.\n",
        "2. Cleaning and standardizing the data.\n",
        "3. Storing it in MongoDB and SQLite.\n",
        "4. Running SQL queries to answer practical questions like “Who sent the most emails?” or “Which email domains are most active?”\n",
        "\n",
        "---\n",
        "\n",
        "## Project Workflow\n",
        "1. **Extract Emails and Dates** – Reads the log file and captures all email addresses with their timestamps.  \n",
        "2. **Transform Data** – Formats dates in a standard `YYYY-MM-DD HH:MM:SS` format and structures the data for database insertion.  \n",
        "3. **Save to MongoDB** – Inserts cleaned data into a MongoDB collection (`user_history`).  \n",
        "4. **Move to SQLite** – Fetches data from MongoDB (or directly uses local data if MongoDB isn’t available) and inserts it into a relational table.  \n",
        "5. **Analyze** – Executes SQL queries to generate insights like unique users, email counts per day, first/last email dates, and top domains.\n",
        "\n",
        "---\n",
        "\n",
        "## Tools & Technologies\n",
        "- **Python 3** – Main scripting and data processing.  \n",
        "- **MongoDB Atlas** – NoSQL storage for document-based data.  \n",
        "- **SQLite** – Relational database for SQL analysis.  \n",
        "- **Regular Expressions (Regex)** – For extracting emails and dates.  \n",
        "- **Google Colab** – Supports file upload, processing, and SQLite download.  \n",
        "\n",
        "---\n",
        "\n",
        "## Project Structure\n",
        "project/\n",
        "│\n",
        "├── mbox.txt # Input server log file\n",
        "├── user_history.db # Generated SQLite database\n",
        "├── pipeline.py # Main Python script\n",
        "└── README.md # This document\n",
        "2. Upload Your Log File\n",
        "\n",
        "The script will prompt you to upload mbox.txt.\n",
        "\n",
        "Ensure log entries look like:\n",
        "\n",
        "From user@example.com Sat Jan  5 09:14:16 2025\n",
        "\n",
        "3. Run the Pipeline\n",
        "python pipeline.py\n",
        "\n",
        "\n",
        "The script will:\n",
        "\n",
        "Extract email-date pairs.\n",
        "\n",
        "Upload to MongoDB (if available).\n",
        "\n",
        "Save the data into SQLite.\n",
        "\n",
        "Run SQL queries to generate insights.\n",
        "\n",
        "\n",
        "Example SQL Queries\n",
        "\n",
        "Here are some sample queries to analyze the data:\n",
        "\n",
        "Count unique email addresses\n",
        "\n",
        "SELECT COUNT(DISTINCT email) FROM user_history;\n",
        "\n",
        "\n",
        "Emails per day\n",
        "\n",
        "SELECT DATE(date), COUNT(*) FROM user_history GROUP BY DATE(date);\n",
        "\n",
        "\n",
        "First and last email per address\n",
        "\n",
        "SELECT email, MIN(date) AS first_email, MAX(date) AS last_email FROM user_history GROUP BY email;\n",
        "\n",
        "\n",
        "Top email domains\n",
        "\n",
        "SELECT SUBSTR(email, INSTR(email,'@')+1) AS domain, COUNT(*) AS count\n",
        "FROM user_history GROUP BY domain ORDER BY count DESC;\n",
        "\n",
        "\n",
        "Total emails\n",
        "\n",
        "SELECT COUNT(*) FROM user_history;\n",
        "\n",
        "\n",
        "Top 5 users by email count\n",
        "\n",
        "SELECT email, COUNT(*) AS count FROM user_history GROUP BY email ORDER BY count DESC LIMIT 5;\n",
        "\n",
        "\n",
        "Emails from Gmail\n",
        "\n",
        "SELECT COUNT(*) FROM user_history WHERE email LIKE '%@gmail.com';\n",
        "\n",
        "\n",
        "Emails after a specific date\n",
        "\n",
        "SELECT * FROM user_history WHERE date > '2025-01-01 00:00:00';\n",
        "\n",
        "\n",
        "Emails per month\n",
        "\n",
        "SELECT STRFTIME('%Y-%m', date) AS month, COUNT(*) FROM user_history GROUP BY month;\n",
        "\n",
        "\n",
        "Custom queries\n",
        "\n",
        "Any other analysis you need can be executed using standard SQL."
      ],
      "metadata": {
        "id": "xbUtqRqJ3WY_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHHpnhjP4xNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}