{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuguvi26/guviproject/blob/main/Copy_of_Server_Log_Data_Extraction_and_User_History_Database_Update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Source code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#   Google Colab ‚Äì Automated Email ETL (Downloads mbox.txt)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q pymongo dnspython\n",
        "\n",
        "import os, re, sqlite3, sys\n",
        "from datetime import datetime\n",
        "from pymongo import MongoClient\n",
        "from google.colab import files\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "LOG_FILE = \"/content/mbox.txt\"\n",
        "MONGO_URI = \"mongodb+srv://testuser:testuser@cluster0.obh30fm.mongodb.net/?appName=Cluster0\"\n",
        "MONGO_DB = \"server_logs\"\n",
        "MONGO_COLLECTION = \"user_history\"\n",
        "SQLITE_DB = \"/content/user_history.db\"\n",
        "\n",
        "# ---------------- STEP 1: DOWNLOAD FILE ----------------\n",
        "print(\"\\n‚¨áÔ∏è Downloading mbox.txt from GitHub...\")\n",
        "!wget -q -O /content/mbox.txt \"https://raw.githubusercontent.com/bhuguvi26/guviproject/main/mbox.txt\"\n",
        "\n",
        "if not os.path.exists(LOG_FILE):\n",
        "    sys.exit(\"‚ùå Failed to download mbox.txt\")\n",
        "\n",
        "print(\"‚úÖ Download complete.\\n\")\n",
        "\n",
        "# ---------------- STEP 2: Extract ----------------\n",
        "def extract_email_date(filepath):\n",
        "    print(\"üîç Extracting email addresses and dates...\")\n",
        "    from_pattern = re.compile(r'^From\\s+([\\w\\.-]+@[\\w\\.-]+)\\s+(.*)')\n",
        "\n",
        "    data, seen = [], set()\n",
        "\n",
        "    with open(filepath, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            m = from_pattern.match(line)\n",
        "            if m:\n",
        "                email, datestr = m.group(1), m.group(2)\n",
        "\n",
        "                # Try common date formats\n",
        "                dt = None\n",
        "                for fmt in [\"%a %b %d %H:%M:%S %Y\", \"%a %b %d %H:%M:%S %z %Y\"]:\n",
        "                    try:\n",
        "                        dt = datetime.strptime(datestr, fmt)\n",
        "                        break\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                if dt:\n",
        "                    key = (email, dt.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "                    if key not in seen:\n",
        "                        seen.add(key)\n",
        "                        data.append({\"email\": key[0], \"date\": key[1]})\n",
        "\n",
        "    print(f\"‚úÖ Extracted {len(data)} email-date pairs.\\n\")\n",
        "    return data\n",
        "\n",
        "extracted = extract_email_date(LOG_FILE)\n",
        "if not extracted:\n",
        "    sys.exit(\"‚ùå No email-date pairs found ‚Äî check your file content!\")\n",
        "\n",
        "# ---------------- STEP 3: MongoDB ----------------\n",
        "def upload_to_mongo(data):\n",
        "    print(\"üåê Connecting to MongoDB Atlas...\")\n",
        "    try:\n",
        "        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
        "        client.admin.command(\"ping\")\n",
        "        print(\"‚úÖ MongoDB connection successful.\")\n",
        "\n",
        "        db = client[MONGO_DB]\n",
        "        col = db[MONGO_COLLECTION]\n",
        "\n",
        "        col.delete_many({})\n",
        "        col.insert_many(data)\n",
        "\n",
        "        print(f\"‚úÖ Inserted {len(data)} documents into MongoDB.\\n\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è MongoDB upload failed: {e}\\nContinuing locally...\")\n",
        "        return False\n",
        "\n",
        "mongo_ok = upload_to_mongo(extracted)\n",
        "\n",
        "# ---------------- STEP 4: Fetch from MongoDB ----------------\n",
        "def fetch_from_mongo():\n",
        "    print(\"üì• Fetching data back from MongoDB...\")\n",
        "    try:\n",
        "        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
        "        db = client[MONGO_DB]\n",
        "        col = db[MONGO_COLLECTION]\n",
        "        recs = list(col.find({}, {\"_id\": 0}))\n",
        "        print(f\"‚úÖ Fetched {len(recs)} records from MongoDB.\\n\")\n",
        "        return recs\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Fetch failed: {e}\")\n",
        "        return []\n",
        "\n",
        "records = fetch_from_mongo() if mongo_ok else extracted\n",
        "\n",
        "# ---------------- STEP 5: SQLite ----------------\n",
        "def save_to_sqlite(records):\n",
        "    print(\"üíæ Saving records to SQLite database...\")\n",
        "    conn = sqlite3.connect(SQLITE_DB)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS user_history(\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            email TEXT NOT NULL,\n",
        "            date TEXT NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cur.executemany(\n",
        "        \"INSERT INTO user_history(email, date) VALUES (?, ?)\",\n",
        "        [(r['email'], r['date']) for r in records]\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"‚úÖ Inserted {len(records)} rows into SQLite.\\n\")\n",
        "\n",
        "save_to_sqlite(records)\n",
        "\n",
        "# ---------------- STEP 6: SQL Queries ----------------\n",
        "def run_sql_queries():\n",
        "    print(\"üìä Running SQL analysis...\\n\")\n",
        "    conn = sqlite3.connect(SQLITE_DB)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    queries = {\n",
        "        \"1Ô∏è‚É£ Unique Emails\": \"SELECT COUNT(DISTINCT email) FROM user_history;\",\n",
        "        \"2Ô∏è‚É£ Emails per Day\": \"SELECT DATE(date), COUNT(*) FROM user_history GROUP BY DATE(date);\",\n",
        "        \"3Ô∏è‚É£ First Email per Address\": \"SELECT email, MIN(date) FROM user_history GROUP BY email LIMIT 5;\",\n",
        "        \"4Ô∏è‚É£ Top Domains\": \"\"\"\n",
        "            SELECT SUBSTR(email, INSTR(email,'@')+1) AS domain, COUNT(*) AS cnt\n",
        "            FROM user_history GROUP BY domain ORDER BY cnt DESC LIMIT 5;\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    for name, q in queries.items():\n",
        "        print(f\"‚û°Ô∏è {name}\")\n",
        "        cur.execute(q)\n",
        "        for row in cur.fetchall():\n",
        "            print(row)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    conn.close()\n",
        "    print(\"\\n‚úÖ SQL analysis complete.\\n\")\n",
        "\n",
        "run_sql_queries()\n",
        "\n",
        "# ---------------- STEP 7: Download SQLite ----------------\n",
        "print(\"üì¶ Downloading SQLite database...\")\n",
        "files.download(SQLITE_DB)\n",
        "print(\"‚úÖ Done! Database saved locally.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-D6eNqeekN-5",
        "outputId": "b09e9ea3-2908-465e-86f3-32c824b57dfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "‚¨áÔ∏è Downloading mbox.txt from GitHub...\n",
            "‚úÖ Download complete.\n",
            "\n",
            "üîç Extracting email addresses and dates...\n",
            "‚úÖ Extracted 1795 email-date pairs.\n",
            "\n",
            "üåê Connecting to MongoDB Atlas...\n",
            "‚úÖ MongoDB connection successful.\n",
            "‚úÖ Inserted 1795 documents into MongoDB.\n",
            "\n",
            "üì• Fetching data back from MongoDB...\n",
            "‚úÖ Fetched 1795 records from MongoDB.\n",
            "\n",
            "üíæ Saving records to SQLite database...\n",
            "‚úÖ Inserted 1795 rows into SQLite.\n",
            "\n",
            "üìä Running SQL analysis...\n",
            "\n",
            "‚û°Ô∏è 1Ô∏è‚É£ Unique Emails\n",
            "(46,)\n",
            "--------------------------------------------------\n",
            "‚û°Ô∏è 2Ô∏è‚É£ Emails per Day\n",
            "('2007-10-18', 13)\n",
            "('2007-10-19', 28)\n",
            "('2007-10-22', 13)\n",
            "('2007-10-23', 26)\n",
            "('2007-10-24', 17)\n",
            "('2007-10-25', 27)\n",
            "('2007-10-26', 29)\n",
            "('2007-10-28', 1)\n",
            "('2007-10-29', 84)\n",
            "('2007-10-30', 55)\n",
            "('2007-10-31', 16)\n",
            "('2007-11-01', 19)\n",
            "('2007-11-02', 2)\n",
            "('2007-11-04', 26)\n",
            "('2007-11-05', 32)\n",
            "('2007-11-06', 70)\n",
            "('2007-11-07', 33)\n",
            "('2007-11-08', 30)\n",
            "('2007-11-09', 27)\n",
            "('2007-11-10', 15)\n",
            "('2007-11-11', 5)\n",
            "('2007-11-12', 10)\n",
            "('2007-11-13', 18)\n",
            "('2007-11-14', 40)\n",
            "('2007-11-15', 38)\n",
            "('2007-11-16', 22)\n",
            "('2007-11-18', 1)\n",
            "('2007-11-19', 52)\n",
            "('2007-11-20', 67)\n",
            "('2007-11-21', 20)\n",
            "('2007-11-23', 1)\n",
            "('2007-11-24', 4)\n",
            "('2007-11-25', 3)\n",
            "('2007-11-26', 12)\n",
            "('2007-11-27', 23)\n",
            "('2007-11-28', 30)\n",
            "('2007-11-29', 52)\n",
            "('2007-11-30', 30)\n",
            "('2007-12-01', 9)\n",
            "('2007-12-02', 4)\n",
            "('2007-12-03', 17)\n",
            "('2007-12-04', 9)\n",
            "('2007-12-05', 9)\n",
            "('2007-12-06', 38)\n",
            "('2007-12-07', 56)\n",
            "('2007-12-08', 2)\n",
            "('2007-12-09', 3)\n",
            "('2007-12-10', 12)\n",
            "('2007-12-11', 32)\n",
            "('2007-12-12', 46)\n",
            "('2007-12-13', 60)\n",
            "('2007-12-14', 56)\n",
            "('2007-12-15', 24)\n",
            "('2007-12-16', 15)\n",
            "('2007-12-17', 58)\n",
            "('2007-12-18', 68)\n",
            "('2007-12-19', 43)\n",
            "('2007-12-20', 56)\n",
            "('2007-12-21', 38)\n",
            "('2007-12-22', 2)\n",
            "('2007-12-23', 1)\n",
            "('2007-12-24', 2)\n",
            "('2007-12-25', 1)\n",
            "('2007-12-26', 3)\n",
            "('2007-12-27', 17)\n",
            "('2007-12-28', 6)\n",
            "('2007-12-29', 4)\n",
            "('2007-12-30', 7)\n",
            "('2007-12-31', 7)\n",
            "('2008-01-01', 2)\n",
            "('2008-01-02', 34)\n",
            "('2008-01-03', 42)\n",
            "('2008-01-04', 20)\n",
            "('2008-01-05', 1)\n",
            "--------------------------------------------------\n",
            "‚û°Ô∏è 3Ô∏è‚É£ First Email per Address\n",
            "('a.fish@lancaster.ac.uk', '2007-10-29 08:03:16')\n",
            "('aaronz@vt.edu', '2007-10-18 13:14:54')\n",
            "('ajpoland@iupui.edu', '2007-10-19 09:24:52')\n",
            "('antranig@caret.cam.ac.uk', '2007-11-06 11:34:04')\n",
            "('arwhyte@umich.edu', '2007-11-06 15:49:36')\n",
            "--------------------------------------------------\n",
            "‚û°Ô∏è 4Ô∏è‚É£ Top Domains\n",
            "('iupui.edu', 535)\n",
            "('umich.edu', 490)\n",
            "('indiana.edu', 178)\n",
            "('caret.cam.ac.uk', 157)\n",
            "('vt.edu', 110)\n",
            "--------------------------------------------------\n",
            "\n",
            "‚úÖ SQL analysis complete.\n",
            "\n",
            "üì¶ Downloading SQLite database...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_325f2b7c-c1e6-4e57-bee9-21ce01553e49\", \"user_history.db\", 98304)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Done! Database saved locally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "IT-2mdHWkNJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReadMe\n",
        "# User History Extraction & Analysis\n",
        "\n",
        "## Overview\n",
        "This project is designed to make sense of email activity recorded in server log files. Logs are usually unstructured and hard to analyze, so the goal here is to extract email addresses and timestamps, store them in databases, and run queries to get actionable insights. In short, it‚Äôs a small but complete data pipeline for tracking historical email activity.\n",
        "\n",
        "---\n",
        "\n",
        "## Problem\n",
        "Server logs contain valuable information about user activity, but the raw data is messy. This project solves that by:\n",
        "1. Extracting every email address along with its associated date and time.\n",
        "2. Cleaning and standardizing the data.\n",
        "3. Storing it in MongoDB and SQLite.\n",
        "4. Running SQL queries to answer practical questions like ‚ÄúWho sent the most emails?‚Äù or ‚ÄúWhich email domains are most active?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## Project Workflow\n",
        "1. **Extract Emails and Dates** ‚Äì Reads the log file and captures all email addresses with their timestamps.  \n",
        "2. **Transform Data** ‚Äì Formats dates in a standard `YYYY-MM-DD HH:MM:SS` format and structures the data for database insertion.  \n",
        "3. **Save to MongoDB** ‚Äì Inserts cleaned data into a MongoDB collection (`user_history`).  \n",
        "4. **Move to SQLite** ‚Äì Fetches data from MongoDB (or directly uses local data if MongoDB isn‚Äôt available) and inserts it into a relational table.  \n",
        "5. **Analyze** ‚Äì Executes SQL queries to generate insights like unique users, email counts per day, first/last email dates, and top domains.\n",
        "\n",
        "---\n",
        "\n",
        "## Tools & Technologies\n",
        "- **Python 3** ‚Äì Main scripting and data processing.  \n",
        "- **MongoDB Atlas** ‚Äì NoSQL storage for document-based data.  \n",
        "- **SQLite** ‚Äì Relational database for SQL analysis.  \n",
        "- **Regular Expressions (Regex)** ‚Äì For extracting emails and dates.  \n",
        "- **Google Colab** ‚Äì Supports file upload, processing, and SQLite download.  \n",
        "\n",
        "---\n",
        "\n",
        "## Project Structure\n",
        "project/\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ mbox.txt # Input server log file\n",
        "‚îú‚îÄ‚îÄ user_history.db # Generated SQLite database\n",
        "‚îú‚îÄ‚îÄ pipeline.py # Main Python script\n",
        "‚îî‚îÄ‚îÄ README.md # This document\n",
        "2. Upload Your Log File\n",
        "\n",
        "The script will prompt you to upload mbox.txt.\n",
        "\n",
        "Ensure log entries look like:\n",
        "\n",
        "From user@example.com Sat Jan  5 09:14:16 2025\n",
        "\n",
        "3. Run the Pipeline\n",
        "python pipeline.py\n",
        "\n",
        "\n",
        "The script will:\n",
        "\n",
        "Extract email-date pairs.\n",
        "\n",
        "Upload to MongoDB (if available).\n",
        "\n",
        "Save the data into SQLite.\n",
        "\n",
        "Run SQL queries to generate insights.\n",
        "\n",
        "\n",
        "Example SQL Queries\n",
        "\n",
        "Here are some sample queries to analyze the data:\n",
        "\n",
        "Count unique email addresses\n",
        "\n",
        "SELECT COUNT(DISTINCT email) FROM user_history;\n",
        "\n",
        "\n",
        "Emails per day\n",
        "\n",
        "SELECT DATE(date), COUNT(*) FROM user_history GROUP BY DATE(date);\n",
        "\n",
        "\n",
        "First and last email per address\n",
        "\n",
        "SELECT email, MIN(date) AS first_email, MAX(date) AS last_email FROM user_history GROUP BY email;\n",
        "\n",
        "\n",
        "Top email domains\n",
        "\n",
        "SELECT SUBSTR(email, INSTR(email,'@')+1) AS domain, COUNT(*) AS count\n",
        "FROM user_history GROUP BY domain ORDER BY count DESC;\n",
        "\n",
        "\n",
        "Total emails\n",
        "\n",
        "SELECT COUNT(*) FROM user_history;\n",
        "\n",
        "\n",
        "Top 5 users by email count\n",
        "\n",
        "SELECT email, COUNT(*) AS count FROM user_history GROUP BY email ORDER BY count DESC LIMIT 5;\n",
        "\n",
        "\n",
        "Emails from Gmail\n",
        "\n",
        "SELECT COUNT(*) FROM user_history WHERE email LIKE '%@gmail.com';\n",
        "\n",
        "\n",
        "Emails after a specific date\n",
        "\n",
        "SELECT * FROM user_history WHERE date > '2025-01-01 00:00:00';\n",
        "\n",
        "\n",
        "Emails per month\n",
        "\n",
        "SELECT STRFTIME('%Y-%m', date) AS month, COUNT(*) FROM user_history GROUP BY month;\n",
        "\n",
        "\n",
        "Custom queries\n",
        "\n",
        "Any other analysis you need can be executed using standard SQL."
      ],
      "metadata": {
        "id": "xbUtqRqJ3WY_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHHpnhjP4xNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}